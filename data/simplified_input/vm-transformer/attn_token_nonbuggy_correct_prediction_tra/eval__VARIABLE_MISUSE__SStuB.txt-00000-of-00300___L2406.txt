
Original sample:

{"has_bug": false, "bug_kind": 0, "bug_kind_name": "NONE", "source_tokens": ["#NEWLINE#", "def __setstate__(", "self", ",", "state", ")", ":", "#NEWLINE#", "#INDENT#", "'Pickle-protocol - set state of the estimator.\\n\\n        We need to rebuild the interpolation function.\\n        '", "#NEWLINE#", "self", ".", "__dict__", ".", "update", "(", "state", ")", "#NEWLINE#", "if", "(", "hasattr", "(", "self", ",", "'_necessary_X_'", ")", "and", "hasattr", "(", "self", ",", "'_necessary_y_'", ")", ")", ":", "#NEWLINE#", "#INDENT#", "self", ".", "_build_f", "(", "self", ".", "_necessary_X_", ",", "self", ".", "_necessary_y_", ")"], "error_location": [0], "repair_targets": [], "repair_candidates": [2, 11, 24, 31, 39, 43, 47, 4, 17], "provenances": [{"datasetProvenance": {"datasetName": "ETHPy150Open", "filepath": "scikit-learn/scikit-learn/sklearn/isotonic.py", "license": "bsd-3-clause", "note": "license: manual_eval"}}], "txt_file": "eval__VARIABLE_MISUSE__SStuB.txt-00000-of-00300", "js_count": 2406, "results": {"model": "transformer", "prob": {"loc": [[0.9777477979660034, 3.042978960365872e-06, 2.78147751942015e-07, 9.162310732335754e-08, 2.7254932319920044e-06, 1.0633171854124157e-07, 2.185518894748384e-07, 2.2567924418126495e-07, 1.7337966085051448e-07, 2.2066528515551909e-07, 1.5312716072912735e-07, 4.831065893995401e-07, 9.720580607108786e-08, 1.9952251406607502e-08, 1.98255392547253e-07, 2.0187210125754973e-08, 3.2029572594183264e-07, 0.0007569050649181008, 4.835712275053083e-07, 2.051331904340259e-07, 1.9597742095811554e-07, 9.18281557460432e-08, 1.6477076769660925e-06, 6.180778200359782e-07, 0.0033701250795274973, 5.342552640286158e-07, 3.6693063520942815e-07, 1.3768787709977914e-07, 5.554443850996904e-05, 4.05638229494798e-06, 1.2316586435190402e-06, 0.013888695277273655, 9.195554184771026e-07, 1.1009037734766025e-06, 3.2577122510701884e-07, 6.019286047376227e-08, 3.075051608902868e-06, 1.6104205258216098e-07, 8.316103361494243e-08, 9.466614159236997e-08, 1.4822470006947697e-07, 1.585334707954189e-08, 1.4210041854312294e-07, 0.0022675481159240007, 1.152161530626472e-06, 7.29923286257872e-08, 2.0308100090460357e-07, 0.0018863375298678875, 1.180873255179904e-06, 4.843657919195721e-08, 2.770617584246793e-07]], "pointer": [[0.0, 0.0, 0.017917528748512268, 0.0, 0.6966850161552429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019209370017051697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22620567679405212, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00721017038449645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005684675648808479, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021377621218562126, 0.0, 0.0, 0.0, 0.0034070275723934174, 0.0, 0.0, 0.0, 0.002302881795912981, 0.0, 0.0, 0.0]], "target": [0.0]}, "loss": [0.02250348962843418, 0.0], "acc": [1.0, 0.0, 0.0, 0.0]}}


All source tokens:

['#NEWLINE#', 'def __setstate__(', 'self', ',', 'state', ')', ':', '#NEWLINE#', '#INDENT#', "'Pickle-protocol - set state of the estimator.\\n\\n        We need to rebuild the interpolation function.\\n        '", '#NEWLINE#', 'self', '.', '__dict__', '.', 'update', '(', 'state', ')', '#NEWLINE#', 'if', '(', 'hasattr', '(', 'self', ',', "'_necessary_X_'", ')', 'and', 'hasattr', '(', 'self', ',', "'_necessary_y_'", ')', ')', ':', '#NEWLINE#', '#INDENT#', 'self', '.', '_build_f', '(', 'self', '.', '_necessary_X_', ',', 'self', '.', '_necessary_y_', ')']


All attention probs:

[0.030050987377762794, 0.018179161474108696, 0.06814424693584442, 0.0249865110963583, 0.05759567767381668, 0.02219078503549099, 0.016532666981220245, 0.01707620732486248, 0.016875142231583595, 0.019322358071804047, 0.02031167596578598, 0.03949931263923645, 0.011430743150413036, 0.015608958899974823, 0.010731898248195648, 0.012082350440323353, 0.011711048893630505, 0.054858237504959106, 0.008417053148150444, 0.01500134076923132, 0.014218196272850037, 0.012749639339745045, 0.009089970029890537, 0.009526734240353107, 0.05243176594376564, 0.009708822704851627, 0.010005971416831017, 0.008164508268237114, 0.01623222604393959, 0.008680010214447975, 0.009203002788126469, 0.04636861011385918, 0.014005469158291817, 0.012692367658019066, 0.01402500830590725, 0.012713944539427757, 0.01693911850452423, 0.01558762602508068, 0.01513689011335373, 0.03269318863749504, 0.007222473155707121, 0.014058465138077736, 0.013453474268317223, 0.03814968466758728, 0.007712577469646931, 0.012713713571429253, 0.011264069005846977, 0.033317748457193375, 0.009028601460158825, 0.011028180830180645, 0.011271573603153229]


Top-k source tokens:

['self', 'state', 'state', 'self', 'self', 'self', 'self', 'self', 'self', '#NEWLINE#']


Top-k attention probs:

[0.06814424693584442, 0.05759567767381668, 0.054858237504959106, 0.05243176594376564, 0.04636861011385918, 0.03949931263923645, 0.03814968466758728, 0.033317748457193375, 0.03269318863749504, 0.030050987377762794]
