
Original sample:

{"has_bug": false, "bug_kind": 0, "bug_kind_name": "NONE", "source_tokens": ["#NEWLINE#", "def train(", "self", ")", ":", "#NEWLINE#", "#INDENT#", "'\\n        Train Linear Regression Algorithm\\n        From f(x) = WX\\n        Find best h(x) = WX similar to f(x)\\n        Output W\\n\\n        mode: batch / stochastic\\n        '", "#NEWLINE#", "if", "(", "self", ".", "status", "!=", "'init'", ")", ":", "#NEWLINE#", "#INDENT#", "print", "(", "'Please load train data and init W first.'", ")", "#NEWLINE#", "return", "self", ".", "W", "#NEWLINE#", "#UNINDENT#", "self", ".", "status", "=", "'train'", "#NEWLINE#", "for", "i", "in", "range", "(", "0", ",", "self", ".", "updates", ")", ":", "#NEWLINE#", "#INDENT#", "if", "(", "self", ".", "feed_mode", "==", "'stochastic'", ")", ":", "#NEWLINE#", "#INDENT#", "stochastic_i", "=", "random", ".", "randint", "(", "0", ",", "(", "self", ".", "data_num", "-", "1", ")", ")", "#NEWLINE#", "x", "=", "self", ".", "train_X", "[", "stochastic_i", "]", "#NEWLINE#", "y", "=", "self", ".", "train_Y", "[", "stochastic_i", "]", "#NEWLINE#", "gradient", "=", "self", ".", "calculate_gradient", "(", "x", ",", "y", ",", "self", ".", "W", ")", "#NEWLINE#", "#UNINDENT#", "else", ":", "#NEWLINE#", "#INDENT#", "gradient", "=", "self", ".", "calculate_gradient", "(", "self", ".", "train_X", ",", "self", ".", "train_Y", ",", "self", ".", "W", ")", "#NEWLINE#", "#UNINDENT#", "if", "(", "np", ".", "linalg", ".", "norm", "(", "gradient", ")", "==", "0", ")", ":", "#NEWLINE#", "#INDENT#", "return", "self", ".", "W", "#NEWLINE#", "#UNINDENT#", "self", ".", "W", "=", "(", "self", ".", "W", "-", "(", "self", ".", "step_ita", "*", "gradient", ")", ")", "#NEWLINE#", "#UNINDENT#", "return", "self", ".", "W"], "error_location": [0], "repair_targets": [], "repair_candidates": [88, 105, 38, 79, 103, 2, 11, 26, 31, 44, 53, 71, 81, 90, 99, 107, 119, 123, 127, 131, 154, 159, 164, 169, 179, 62, 85, 94, 97, 117, 145, 173], "provenances": [{"datasetProvenance": {"datasetName": "ETHPy150Open", "filepath": "fukuball/fuku-ml/FukuML/LogisticRegression.py", "license": "mit", "note": "license: bigquery_api"}}], "txt_file": "eval__VARIABLE_MISUSE__SStuB.txt-00000-of-00300", "js_count": 1230, "results": {"model": "transformer", "prob": {"loc": [[0.9839276671409607, 2.2320448351820232e-06, 2.1721231036053723e-08, 4.9262167678421065e-09, 1.168362473435991e-08, 1.433001806105949e-08, 1.1414583944713286e-08, 1.0643256054265748e-08, 1.1505953523283097e-08, 9.46432265891417e-09, 1.3072690485671501e-08, 1.933575185830705e-05, 1.8270398527420184e-08, 3.497064193780375e-09, 8.448141741723703e-09, 7.360934972666655e-08, 4.253344343396748e-09, 8.693369579759747e-09, 4.741410819519842e-09, 2.7988225070885164e-09, 8.15470690973541e-10, 8.99469831949773e-09, 4.899261512036901e-06, 9.171492010295879e-09, 5.6910383072761306e-09, 1.984856368153487e-08, 2.84558450402983e-06, 1.1460571158750099e-08, 7.412355174096774e-09, 8.289198660804686e-09, 4.7685553283827176e-09, 8.254063459389727e-07, 5.72569414103441e-09, 8.396447981340316e-09, 2.491069892585074e-07, 8.491313252534383e-08, 1.737620536346185e-08, 1.1571891889161634e-08, 1.4195788025972433e-05, 3.4321532282888256e-09, 1.880410271937194e-09, 1.6072823427748517e-07, 1.115887926061987e-06, 8.938654261214651e-09, 2.0796417175006354e-06, 7.922340472532596e-08, 2.281299771311751e-07, 1.3904875473258471e-08, 1.0402622763194813e-07, 3.3624409923049825e-08, 4.844804113446344e-09, 9.133644063297197e-09, 1.3160578404836087e-08, 3.1471718102693558e-06, 3.8777489663743836e-08, 6.496692517288238e-09, 3.364115386261801e-08, 2.1951695998723153e-06, 1.567270260238729e-07, 1.2663292636716506e-07, 3.853241636875282e-08, 1.3765729001136151e-08, 3.087442337346147e-06, 1.759659795652624e-08, 2.5780138912523398e-08, 1.8591347128449343e-08, 5.828045246403235e-08, 2.1035810959801893e-07, 6.1745809034619015e-06, 1.894259504808815e-08, 1.908162694519433e-08, 4.026783699373482e-06, 4.463560188128213e-08, 6.9173431427316245e-09, 1.7749641756381607e-07, 3.574461516109295e-05, 3.0585445642827835e-07, 7.491813391879987e-08, 1.766684931681084e-08, 4.3388990889070556e-06, 5.073177877790158e-09, 5.0329085752309766e-06, 3.244587887252237e-08, 1.5979059186932432e-09, 1.2674571614468277e-08, 0.0022589699365198612, 1.0255497073785591e-07, 2.5765688249634877e-08, 1.2286742276046425e-05, 1.695905993415181e-08, 7.56516828914755e-06, 3.2895030699364725e-08, 7.073728713891114e-09, 7.466630869146229e-09, 0.001317884773015976, 7.19663404424864e-08, 4.466106062750441e-08, 1.990894816117361e-06, 9.116273957943122e-09, 7.619348707521567e-08, 3.983716467814702e-08, 8.41684411057031e-09, 2.948912367628509e-07, 0.0001149288218584843, 4.488557081572253e-08, 0.0018758547957986593, 2.3645638336233787e-08, 1.0322190973965917e-05, 1.4221112110135437e-07, 7.961619274965415e-08, 7.864752404884712e-08, 5.6968406880741895e-08, 3.632567313616164e-08, 3.522188407600879e-08, 3.4916006086405105e-08, 3.1498110786287725e-08, 1.5164355104957394e-08, 4.916893431072822e-06, 8.43914449433214e-08, 1.573042709424044e-07, 6.83678251789388e-08, 2.6826231902532527e-08, 1.4834795081242191e-07, 2.916644916695077e-05, 1.1832294433133939e-07, 1.4385665991767382e-08, 1.533489495386675e-07, 1.7172351363115013e-05, 1.1859552984105903e-07, 1.0652157556023667e-07, 1.7527183615584363e-07, 1.411875018675346e-05, 1.373154105976937e-07, 1.7819861852785834e-07, 1.0237868508511383e-07, 1.0161297581134932e-07, 1.0100873026885893e-07, 7.542266899918104e-08, 3.754524158239292e-08, 1.73708212969359e-06, 6.342533254155569e-08, 1.633642376930311e-08, 1.1816551648280438e-07, 1.0895384150444443e-08, 5.204814783610345e-07, 0.001146851689554751, 1.9386591887382565e-08, 1.448150470650944e-07, 1.2526926184364129e-05, 1.0551215012810644e-07, 5.935780791332945e-07, 1.1138587296954938e-07, 3.403978254823414e-08, 7.07451164316808e-09, 7.625335274497047e-05, 5.670854719141971e-08, 7.093782983247365e-08, 1.7365032078942022e-07, 8.79972503753379e-08, 1.3509411189716047e-07, 2.6574147327096398e-08, 5.996432719257427e-07, 7.55809423935716e-08, 1.396674775833162e-07, 2.7458459953777492e-05, 1.1728687354661815e-07, 2.7223009624322003e-07, 6.82121736872432e-08, 3.161392783113115e-07, 1.4417702914215624e-05, 1.259317770063717e-07, 2.5370001210944793e-08, 3.895741329529301e-08, 0.008991999551653862, 2.597719515051722e-07, 7.271542301623413e-08, 3.203809839646965e-08, 2.056944659045712e-08, 2.3148418737406473e-09, 1.4009785445523448e-05, 5.1269932299646825e-08, 2.8273643692955375e-07]], "pointer": [[0.0, 0.0, 0.010463464073836803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0003657494962681085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0005828740540891886, 0.0, 0.0, 0.0, 0.0, 0.001272832858376205, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3438740670681, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0006107704248279333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000560932676307857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1103384867310524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0003235383192077279, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06450530141592026, 0.0, 9.371065243612975e-05, 0.0, 0.0, 0.0, 0.001279311254620552, 0.0, 0.0, 0.0678124949336052, 0.0, 0.00015524189802818, 0.0, 0.0, 0.0, 0.001104417722672224, 0.0, 0.0, 0.14332765340805054, 0.0, 0.00015041395090520382, 0.0, 0.0, 0.0, 0.005987260956317186, 0.0, 0.0004619697865564376, 0.0, 0.0008859540685079992, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20969755947589874, 0.0, 0.00034178909845650196, 0.0, 0.0, 0.0, 0.00028607374406419694, 0.0, 0.0, 0.0, 0.0005533496732823551, 0.0, 0.0, 0.0, 0.0009410721249878407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02414483204483986, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009534009732306004, 0.0, 0.0, 0.0, 0.0, 0.004884309135377407, 0.0, 0.0, 0.0, 0.0, 0.0005223379121161997, 0.0, 0.0, 0.0, 0.0, 0.00024302068050019443, 0.0, 0.0, 0.0, 0.0030170853715389967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002587215567473322, 0.0, 0.0]], "target": [0.0]}, "loss": [0.016203146427869797, 0.0], "acc": [1.0, 0.0, 0.0, 0.0]}}


All source tokens:

['#NEWLINE#', 'def train(', 'self', ')', ':', '#NEWLINE#', '#INDENT#', "'\\n        Train Linear Regression Algorithm\\n        From f(x) = WX\\n        Find best h(x) = WX similar to f(x)\\n        Output W\\n\\n        mode: batch / stochastic\\n        '", '#NEWLINE#', 'if', '(', 'self', '.', 'status', '!=', "'init'", ')', ':', '#NEWLINE#', '#INDENT#', 'print', '(', "'Please load train data and init W first.'", ')', '#NEWLINE#', 'return', 'self', '.', 'W', '#NEWLINE#', '#UNINDENT#', 'self', '.', 'status', '=', "'train'", '#NEWLINE#', 'for', 'i', 'in', 'range', '(', '0', ',', 'self', '.', 'updates', ')', ':', '#NEWLINE#', '#INDENT#', 'if', '(', 'self', '.', 'feed_mode', '==', "'stochastic'", ')', ':', '#NEWLINE#', '#INDENT#', 'stochastic_i', '=', 'random', '.', 'randint', '(', '0', ',', '(', 'self', '.', 'data_num', '-', '1', ')', ')', '#NEWLINE#', 'x', '=', 'self', '.', 'train_X', '[', 'stochastic_i', ']', '#NEWLINE#', 'y', '=', 'self', '.', 'train_Y', '[', 'stochastic_i', ']', '#NEWLINE#', 'gradient', '=', 'self', '.', 'calculate_gradient', '(', 'x', ',', 'y', ',', 'self', '.', 'W', ')', '#NEWLINE#', '#UNINDENT#', 'else', ':', '#NEWLINE#', '#INDENT#', 'gradient', '=', 'self', '.', 'calculate_gradient', '(', 'self', '.', 'train_X', ',', 'self', '.', 'train_Y', ',', 'self', '.', 'W', ')', '#NEWLINE#', '#UNINDENT#', 'if', '(', 'np', '.', 'linalg', '.', 'norm', '(', 'gradient', ')', '==', '0', ')', ':', '#NEWLINE#', '#INDENT#', 'return', 'self', '.', 'W', '#NEWLINE#', '#UNINDENT#', 'self', '.', 'W', '=', '(', 'self', '.', 'W', '-', '(', 'self', '.', 'step_ita', '*', 'gradient', ')', ')', '#NEWLINE#', '#UNINDENT#', 'return', 'self', '.', 'W']


All attention probs:

[0.011959672905504704, 0.007794513832777739, 0.04406538978219032, 0.013860147446393967, 0.009984204545617104, 0.009030694141983986, 0.007594397757202387, 0.010165050625801086, 0.007006342057138681, 0.006720951292663813, 0.006159468553960323, 0.02035978063941002, 0.005685271229594946, 0.005533986259251833, 0.00517237139865756, 0.00924684852361679, 0.005589235574007034, 0.005530030932277441, 0.006242791190743446, 0.006788571365177631, 0.006031343713402748, 0.004784987308084965, 0.012333189137279987, 0.004624771885573864, 0.005460951942950487, 0.0046751187182962894, 0.011198779568076134, 0.0028577183838933706, 0.003430155571550131, 0.004648467060178518, 0.004450808744877577, 0.008988765999674797, 0.003434069687500596, 0.004407061729580164, 0.003473816439509392, 0.0039364732801914215, 0.004944147076457739, 0.00530163524672389, 0.021254874765872955, 0.004884767346084118, 0.0034315355587750673, 0.0032787045929580927, 0.00542391138151288, 0.003524079453200102, 0.010303315706551075, 0.0030461461283266544, 0.003759796265512705, 0.0021837546955794096, 0.002950419904664159, 0.004228862002491951, 0.0034965495578944683, 0.002850188408046961, 0.0021545214112848043, 0.012392339296638966, 0.0022123169619590044, 0.002984100254252553, 0.002431991510093212, 0.00399653660133481, 0.0028505248483270407, 0.003005219390615821, 0.0031045638024806976, 0.0034119603224098682, 0.013562336564064026, 0.002992033725604415, 0.0036942774895578623, 0.0022481309715658426, 0.002723404671996832, 0.003005163511261344, 0.004920333623886108, 0.0031737308017909527, 0.0026530141476541758, 0.008649333380162716, 0.0020787008106708527, 0.003561554243788123, 0.002734588459134102, 0.005073116160929203, 0.0021854424849152565, 0.0017252081306651235, 0.004146336577832699, 0.011914188973605633, 0.003080817172303796, 0.007140324451029301, 0.0019900898914784193, 0.0036584394983947277, 0.0024275074247270823, 0.010550604201853275, 0.002894414123147726, 0.00435587577521801, 0.011469988152384758, 0.002995785791426897, 0.007802356965839863, 0.0019330624490976334, 0.003044047160074115, 0.0030195950530469418, 0.011709678918123245, 0.00258051510900259, 0.003705304116010666, 0.010357601568102837, 0.0063833496533334255, 0.006677161902189255, 0.001976495375856757, 0.0032587817404419184, 0.0033198315650224686, 0.011804021894931793, 0.0032962332479655743, 0.016585495322942734, 0.0022877631708979607, 0.007887945510447025, 0.001837902469560504, 0.0028509560506790876, 0.0021627373062074184, 0.0047189174219965935, 0.0041863638907670975, 0.00753258541226387, 0.002729888306930661, 0.0030334563925862312, 0.0029068393632769585, 0.013735960237681866, 0.0029996552038937807, 0.0062194098718464375, 0.0019686894956976175, 0.0021812866907566786, 0.002585768233984709, 0.008051403798162937, 0.0024212345015257597, 0.0026307525113224983, 0.0034840437583625317, 0.007681155111640692, 0.0022135009057819843, 0.0028501178603619337, 0.0031161357183009386, 0.007203408516943455, 0.001899171620607376, 0.0024980991147458553, 0.0016593912150710821, 0.003941608592867851, 0.0033447311725467443, 0.004270210396498442, 0.0030580011662095785, 0.00552747305482626, 0.0017511338228359818, 0.0024295293260365725, 0.0019458056194707751, 0.003879467723891139, 0.0031961530912667513, 0.018860124051570892, 0.0021786652505397797, 0.002521353540942073, 0.00441422313451767, 0.0022230411414057016, 0.0030940277501940727, 0.0035091054160147905, 0.004326903726905584, 0.006158726755529642, 0.014610296115279198, 0.003842257661744952, 0.0029552211053669453, 0.004843566101044416, 0.005393821280449629, 0.006812428589910269, 0.0025927650276571512, 0.005162336863577366, 0.0027310389559715986, 0.002645848086103797, 0.009061779826879501, 0.002056865254417062, 0.0035796004813164473, 0.0026516770012676716, 0.0026350729167461395, 0.008120479062199593, 0.0019802444148808718, 0.0030922740697860718, 0.0029403269290924072, 0.03424723446369171, 0.003374154679477215, 0.0026972447521984577, 0.004069076851010323, 0.003637356450781226, 0.004200133495032787, 0.009888952597975731, 0.0027647942770272493, 0.003557093907147646]


Top-k source tokens:

['self', 'gradient', 'i', 'self', 'gradient', 'y', 'self', ')', 'gradient', 'stochastic_i']


Top-k attention probs:

[0.04406538978219032, 0.03424723446369171, 0.021254874765872955, 0.02035978063941002, 0.018860124051570892, 0.016585495322942734, 0.014610296115279198, 0.013860147446393967, 0.013735960237681866, 0.013562336564064026]
