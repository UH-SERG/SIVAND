
Original sample:

{"has_bug": false, "bug_kind": 0, "bug_kind_name": "NONE", "source_tokens": ["#NEWLINE#", "def predictProba(", "self", ",", "message", ")", ":", "#NEWLINE#", "#INDENT#", "'\\n        This returns probability estimate for each class.\\n        '", "#NEWLINE#", "return", "self", ".", "_posNegPrbDic"], "error_location": [0], "repair_targets": [], "repair_candidates": [4, 2, 12], "provenances": [{"datasetProvenance": {"datasetName": "ETHPy150Open", "filepath": "dssg/ushine-learning/dssg/classifier.py", "license": "mit", "note": "license: bigquery_api"}}], "txt_file": "eval__VARIABLE_MISUSE__SStuB.txt-00000-of-00300", "js_count": 1247, "results": {"model": "transformer", "prob": {"loc": [[0.8071795105934143, 9.003311424748972e-05, 6.782743730582297e-05, 4.673369460306276e-07, 7.797510988893919e-06, 5.171413590687735e-07, 3.1835693334869575e-06, 3.498000751278596e-06, 2.543488790252013e-06, 8.688255547895096e-06, 1.9905224689864554e-05, 1.0643931318554678e-06, 0.1926022171974182, 4.0712880036153365e-06, 8.609829819761217e-06]], "pointer": [[0.0, 0.0, 0.0021456873510032892, 0.0, 0.9939529299736023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003901379881426692, 0.0, 0.0]], "target": [0.0]}, "loss": [0.21420903503894806, 0.0], "acc": [1.0, 0.0, 0.0, 0.0]}}


All source tokens:

['#NEWLINE#', 'def predictProba(', 'self', ',', 'message', ')', ':', '#NEWLINE#', '#INDENT#', "'\\n        This returns probability estimate for each class.\\n        '", '#NEWLINE#', 'return', 'self', '.', '_posNegPrbDic']


All attention probs:

[0.07801678776741028, 0.03689727932214737, 0.12825018167495728, 0.05302802100777626, 0.11677815020084381, 0.05135512724518776, 0.050520870834589005, 0.04864979907870293, 0.03940903767943382, 0.06259635090827942, 0.05375492945313454, 0.059698592871427536, 0.14761163294315338, 0.03373709321022034, 0.039696104824543]


Top-k source tokens:

['self', 'self', 'message', '#NEWLINE#', "'\\n        This returns probability estimate for each class.\\n        '", 'return', '#NEWLINE#', ',', ')', ':']


Top-k attention probs:

[0.14761163294315338, 0.12825018167495728, 0.11677815020084381, 0.07801678776741028, 0.06259635090827942, 0.059698592871427536, 0.05375492945313454, 0.05302802100777626, 0.05135512724518776, 0.050520870834589005]
