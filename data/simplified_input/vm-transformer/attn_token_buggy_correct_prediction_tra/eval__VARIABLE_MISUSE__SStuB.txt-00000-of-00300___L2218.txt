
Original sample:

{"has_bug": true, "bug_kind": 1, "bug_kind_name": "VARIABLE_MISUSE", "source_tokens": ["#NEWLINE#", "def fit(", "self", ",", "X", ",", "y", ")", ":", "#NEWLINE#", "#INDENT#", "\"Fit the model using X as training data and y as target values\\n\\n        Parameters\\n        ----------\\n        X : {array-like, sparse matrix, BallTree, KDTree}\\n            Training data. If array or matrix, shape [n_samples, n_features],\\n            or [n_samples, n_samples] if metric='precomputed'.\\n\\n        y : {array-like, sparse matrix}\\n            Target values of shape = [n_samples] or [n_samples, n_outputs]\\n\\n        \"", "#NEWLINE#", "if", "(", "not", "isinstance", "(", "X", ",", "(", "KDTree", ",", "BallTree", ")", ")", ")", ":", "#NEWLINE#", "#INDENT#", "(", "X", ",", "y", ")", "=", "check_X_y", "(", "X", ",", "y", ",", "'csr'", ",", "multi_output", "=", "True", ")", "#NEWLINE#", "#UNINDENT#", "if", "(", "(", "y", ".", "ndim", "==", "1", ")", "or", "(", "(", "y", ".", "ndim", "==", "2", ")", "and", "(", "y", ".", "shape", "[", "1", "]", "==", "1", ")", ")", ")", ":", "#NEWLINE#", "#INDENT#", "if", "(", "y", ".", "ndim", "!=", "1", ")", ":", "#NEWLINE#", "#INDENT#", "warnings", ".", "warn", "(", "'A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().'", ",", "DataConversionWarning", ",", "stacklevel", "=", "2", ")", "#NEWLINE#", "#UNINDENT#", "self", ".", "outputs_2d_", "=", "False", "#NEWLINE#", "y", "=", "y", ".", "reshape", "(", "(", "(", "-", "1", ")", ",", "1", ")", ")", "#NEWLINE#", "#UNINDENT#", "else", ":", "#NEWLINE#", "#INDENT#", "self", ".", "outputs_2d_", "=", "True", "#NEWLINE#", "#UNINDENT#", "check_classification_targets", "(", "y", ")", "#NEWLINE#", "self", ".", "classes_", "=", "[", "]", "#NEWLINE#", "self", ".", "_y", "=", "np", ".", "empty", "(", "y", ".", "shape", ",", "dtype", "=", "np", ".", "int", ")", "#NEWLINE#", "for", "k", "in", "range", "(", "self", ".", "_y", ".", "shape", "[", "1", "]", ")", ":", "#NEWLINE#", "#INDENT#", "(", "classes", ",", "self", ".", "_y", "[", ":", ",", "k", "]", ")", "=", "np", ".", "unique", "(", "y", "[", ":", ",", "k", "]", ",", "return_inverse", "=", "True", ")", "#NEWLINE#", "self", ".", "classes_", ".", "append", "(", "classes", ")", "#NEWLINE#", "#UNINDENT#", "if", "(", "not", "y", ".", "outputs_2d_", ")", ":", "#NEWLINE#", "#INDENT#", "self", ".", "classes_", "=", "self", ".", "classes_", "[", "0", "]", "#NEWLINE#", "self", ".", "_y", "=", "self", ".", "_y", ".", "ravel", "(", ")", "#NEWLINE#", "#UNINDENT#", "return", "self", ".", "_fit", "(", "X", ")"], "error_location": [233], "repair_targets": [2, 109, 136, 148, 155, 179, 194, 220, 240, 244, 251, 255, 265], "repair_candidates": [6, 33, 40, 53, 62, 70, 86, 115, 117, 145, 163, 208, 233, 2, 109, 136, 148, 155, 179, 194, 220, 240, 244, 251, 255, 265, 4, 18, 31, 38, 269, 175, 200, 212, 192, 226], "provenances": [{"datasetProvenance": {"datasetName": "ETHPy150Open", "filepath": "scikit-learn/scikit-learn/sklearn/neighbors/base.py", "license": "bsd-3-clause", "note": "license: manual_eval"}}], "txt_file": "eval__VARIABLE_MISUSE__SStuB.txt-00000-of-00300", "js_count": 2218, "results": {"model": "transformer", "prob": {"loc": [[0.0036970425862818956, 4.042145107518991e-08, 3.1706711922652175e-08, 4.1727918342893133e-10, 1.1290551604759003e-08, 4.916300144763852e-10, 2.7997879215035937e-07, 2.817512889663476e-10, 7.658560452483698e-10, 1.5091771166098056e-09, 9.890820384939047e-10, 7.499640020114384e-09, 1.5431402822230211e-09, 1.4296126504831363e-09, 5.844782879904642e-09, 3.54359341869781e-09, 1.5080837689751547e-09, 6.5808229976482835e-09, 0.00013517250772565603, 2.8135724861044764e-09, 3.688762184594907e-09, 2.4774982421149616e-07, 1.870968269201967e-09, 6.741835534285201e-08, 6.019258425027374e-10, 2.4804686082902094e-10, 2.6711469147677747e-10, 1.1921053033603357e-09, 9.508208664854578e-10, 5.477504005924061e-10, 3.8767300480913036e-10, 3.2449577247462e-08, 2.7203983510304397e-10, 3.175040674818774e-08, 1.3382450703147697e-10, 2.817921451736538e-10, 2.8913346716841204e-10, 7.626248077485798e-09, 8.641643034934532e-06, 1.3634540163565134e-09, 7.028390245977789e-05, 1.0779762638080115e-09, 5.56468782164643e-09, 7.563533133136957e-10, 4.090446203974807e-09, 1.4526189140440238e-09, 1.1299233548811571e-08, 2.5809241410712502e-09, 2.3055344300360048e-09, 1.150247674885918e-09, 6.716441292020647e-10, 4.136356257511409e-10, 2.2531379428869514e-08, 6.1834766711399425e-06, 3.2096849622575974e-09, 1.8842860605161604e-10, 3.440034368473732e-10, 5.750656395520082e-09, 1.2655282377593835e-09, 3.5980907142629803e-10, 3.870216147561223e-09, 5.479805498254109e-09, 1.56619989866158e-05, 3.1278544199153657e-09, 1.5607730385891472e-10, 1.1835323832087852e-09, 1.760646028969859e-08, 3.2596000898443833e-10, 1.0573238951039343e-09, 8.946696161693524e-10, 5.279337074171053e-06, 2.3341670818410876e-09, 6.514467854046302e-10, 1.494222967579617e-09, 6.7511609636028425e-09, 5.022779414609602e-10, 9.6619401368514e-10, 1.6067199126723608e-08, 4.372583184419909e-09, 1.960432483016916e-09, 1.3804497545066852e-09, 2.4540975918085906e-09, 1.7362109527851999e-09, 9.398635203439198e-10, 8.348405300395711e-10, 1.4842282070048896e-08, 7.875211849750485e-06, 2.6931972207933086e-09, 1.9398030404182975e-10, 7.871824858618481e-10, 9.151776225735375e-09, 7.135160795535e-10, 3.76448783256933e-09, 1.962853213299809e-09, 1.2295903184522672e-09, 1.7668113638791283e-09, 4.195776504012372e-10, 4.2884817919031093e-10, 3.866305942068493e-09, 2.6842499778467754e-08, 2.9201276952051103e-09, 2.8891695702526476e-09, 7.864875972707353e-10, 2.1903004410184224e-10, 8.934146200623161e-10, 9.11608495357541e-08, 3.149105198829716e-09, 2.5602679976088893e-09, 2.2166950497393145e-09, 1.495931911676962e-08, 7.971940330087079e-10, 2.4783591845434216e-10, 4.058543723317598e-09, 3.379903290579023e-08, 3.586588137594049e-09, 7.849148175864684e-08, 1.734900112460025e-09, 7.816385760861522e-08, 1.4971128781127163e-09, 1.7581844313774297e-10, 2.2217576667316052e-09, 6.260354901144183e-09, 5.793596358216746e-08, 2.4455375502441257e-09, 2.371780816190494e-08, 4.483641458108423e-09, 4.25200363807221e-09, 5.8881219899831194e-08, 9.826067071116995e-09, 5.5461133463552414e-09, 3.6568879036025237e-09, 4.058543723317598e-09, 4.88091922434819e-09, 2.9742428520052044e-09, 2.844187330097725e-09, 1.6472263553168887e-09, 1.4315098439965368e-08, 7.560072012857688e-10, 1.693317708273412e-10, 7.0141270569479275e-09, 5.47781233706246e-08, 2.7525617340984354e-09, 1.8741503904351475e-09, 3.300685280649418e-10, 8.597659473252861e-09, 6.227610720088705e-05, 3.5847005364075812e-09, 2.2522257392409983e-09, 3.109463619921371e-08, 7.648662814219165e-10, 2.6667740238295323e-10, 1.0575503139875764e-08, 8.535639040019305e-08, 4.932052988237956e-09, 2.6141580011795895e-09, 2.4174807933263764e-08, 3.206379051157171e-10, 1.4273014714571985e-10, 9.4779351034191e-10, 3.2958558104922986e-09, 2.7077842190692536e-09, 1.8208595742308376e-09, 1.1345918649041664e-09, 8.699140016688034e-05, 4.992456670294132e-09, 1.5070774628256345e-09, 7.550676195400285e-10, 2.6465776237216687e-09, 9.662423750000926e-09, 2.3431832687492715e-06, 5.7100417727440345e-09, 5.867256014369104e-09, 4.121065710904759e-09, 3.41950801008295e-09, 3.463698661221315e-09, 1.7458086531974004e-08, 2.6202914837902824e-10, 8.854633137822532e-10, 4.475928250258221e-08, 0.00031441557803191245, 4.828666799738812e-09, 3.671535020455252e-10, 4.353285287805875e-09, 1.5490796423378583e-09, 2.51544189922015e-08, 1.3796984887903818e-07, 2.962003087247922e-09, 9.373612108731777e-09, 6.3427094687540375e-09, 3.424377226224351e-09, 1.7486405656796933e-09, 9.086026930837932e-10, 4.530844762484776e-06, 1.2267322713199746e-09, 1.4197398741089273e-06, 1.397673532466115e-09, 1.5379679474403218e-10, 1.3501839646323788e-08, 9.567264314114254e-09, 1.647722847053501e-09, 4.393540621094871e-06, 7.954746861216222e-10, 6.521815865134784e-10, 1.4764570677172628e-09, 1.3790319997042388e-08, 1.0748432144325193e-09, 7.901216902972408e-10, 6.002557118023333e-09, 0.00045780237996950746, 2.6792065455083502e-08, 5.540107927970439e-09, 3.2835698604571917e-09, 1.9594888726715e-05, 1.558866480344534e-09, 1.1558094481500802e-09, 1.3724608116660875e-09, 3.8377110378462476e-09, 9.711390447364465e-08, 7.621449249484158e-09, 1.975642538454281e-09, 1.6505160260749108e-07, 3.7477648762163085e-10, 1.7937780427690342e-10, 8.626926950583425e-10, 6.256889450995118e-10, 5.82292924988792e-09, 2.7767631763708778e-05, 8.085508262922758e-09, 2.948227662002978e-09, 3.15697135100379e-09, 1.5706524969516522e-09, 3.245849100608211e-08, 1.1707846248043552e-08, 0.9940302968025208, 3.5220306671135404e-09, 2.384780373176909e-09, 2.0684385315661302e-09, 6.364523130741873e-09, 2.1207844369541817e-09, 1.8127855883065536e-09, 1.0533073435681217e-07, 1.1817684608672607e-09, 2.690992984000218e-10, 1.0420425411439282e-08, 0.00010660156840458512, 1.372644109487453e-09, 2.429431655848191e-10, 5.079529330487276e-08, 8.741176884541346e-07, 4.088464677920456e-09, 1.3510744745204306e-09, 1.2115910408283526e-07, 4.85004147954271e-10, 1.3087166073066925e-10, 3.5424733368927264e-08, 3.339644308653078e-06, 6.075893677071065e-10, 2.319250347326829e-10, 6.98131663590118e-10, 4.759391769582066e-10, 6.507398779831419e-07, 9.52086320893386e-09, 5.547869275090989e-09, 6.012893294382593e-09, 1.6220162990521203e-09, 1.1593886029004352e-07, 1.2282682648745435e-09, 2.4929724951050503e-09, 8.199548595655415e-09, 0.0009274446638301015, 1.5331780289784547e-08]], "pointer": [[0.0, 0.0, 0.044565599411726, 0.0, 0.00038615611265413463, 0.0, 0.0004148808366153389, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6563580831862055e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00019604575936682522, 0.0, 0.00047858862672001123, 0.0, 0.0, 0.0, 0.0, 2.6425646865391172e-05, 0.0, 6.031767497916007e-06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.684880771790631e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.021661955746822e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.019471321546007e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.646890738513321e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0006628854316659272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00044537504436448216, 0.0, 1.4592654224543367e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000562406494282186, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.021677755692508e-05, 0.0, 0.0, 0.001839969540014863, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011071158573031425, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00045720909838564694, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.747296331217512e-05, 0.0, 0.0, 0.0, 0.015759943053126335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008394146338105202, 0.0, 0.07130584120750427, 0.0, 0.0, 0.0, 0.0, 0.0, 6.52707603876479e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.326742343138903e-05, 0.0, 0.0, 0.0, 7.952310625114478e-06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20098361372947693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0018772890325635672, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.104704930796288e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08955670893192291, 0.0, 0.0, 0.0, 0.39514490962028503, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08678144216537476, 0.0, 0.0, 0.0, 0.036743275821208954, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03166130557656288, 0.0, 0.0, 0.0, 0.0002586760092526674, 0.0]], "target": [0.9866390824317932]}, "loss": [0.005987564101815224, 0.013450977392494678], "acc": [0.0, 1.0, 1.0, 1.0]}}


All source tokens:

['#NEWLINE#', 'def fit(', 'self', ',', 'X', ',', 'y', ')', ':', '#NEWLINE#', '#INDENT#', '"Fit the model using X as training data and y as target values\\n\\n        Parameters\\n        ----------\\n        X : {array-like, sparse matrix, BallTree, KDTree}\\n            Training data. If array or matrix, shape [n_samples, n_features],\\n            or [n_samples, n_samples] if metric=\'precomputed\'.\\n\\n        y : {array-like, sparse matrix}\\n            Target values of shape = [n_samples] or [n_samples, n_outputs]\\n\\n        "', '#NEWLINE#', 'if', '(', 'not', 'isinstance', '(', 'X', ',', '(', 'KDTree', ',', 'BallTree', ')', ')', ')', ':', '#NEWLINE#', '#INDENT#', '(', 'X', ',', 'y', ')', '=', 'check_X_y', '(', 'X', ',', 'y', ',', "'csr'", ',', 'multi_output', '=', 'True', ')', '#NEWLINE#', '#UNINDENT#', 'if', '(', '(', 'y', '.', 'ndim', '==', '1', ')', 'or', '(', '(', 'y', '.', 'ndim', '==', '2', ')', 'and', '(', 'y', '.', 'shape', '[', '1', ']', '==', '1', ')', ')', ')', ':', '#NEWLINE#', '#INDENT#', 'if', '(', 'y', '.', 'ndim', '!=', '1', ')', ':', '#NEWLINE#', '#INDENT#', 'warnings', '.', 'warn', '(', "'A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().'", ',', 'DataConversionWarning', ',', 'stacklevel', '=', '2', ')', '#NEWLINE#', '#UNINDENT#', 'self', '.', 'outputs_2d_', '=', 'False', '#NEWLINE#', 'y', '=', 'y', '.', 'reshape', '(', '(', '(', '-', '1', ')', ',', '1', ')', ')', '#NEWLINE#', '#UNINDENT#', 'else', ':', '#NEWLINE#', '#INDENT#', 'self', '.', 'outputs_2d_', '=', 'True', '#NEWLINE#', '#UNINDENT#', 'check_classification_targets', '(', 'y', ')', '#NEWLINE#', 'self', '.', 'classes_', '=', '[', ']', '#NEWLINE#', 'self', '.', '_y', '=', 'np', '.', 'empty', '(', 'y', '.', 'shape', ',', 'dtype', '=', 'np', '.', 'int', ')', '#NEWLINE#', 'for', 'k', 'in', 'range', '(', 'self', '.', '_y', '.', 'shape', '[', '1', ']', ')', ':', '#NEWLINE#', '#INDENT#', '(', 'classes', ',', 'self', '.', '_y', '[', ':', ',', 'k', ']', ')', '=', 'np', '.', 'unique', '(', 'y', '[', ':', ',', 'k', ']', ',', 'return_inverse', '=', 'True', ')', '#NEWLINE#', 'self', '.', 'classes_', '.', 'append', '(', 'classes', ')', '#NEWLINE#', '#UNINDENT#', 'if', '(', 'not', 'y', '.', 'outputs_2d_', ')', ':', '#NEWLINE#', '#INDENT#', 'self', '.', 'classes_', '=', 'self', '.', 'classes_', '[', '0', ']', '#NEWLINE#', 'self', '.', '_y', '=', 'self', '.', '_y', '.', 'ravel', '(', ')', '#NEWLINE#', '#UNINDENT#', 'return', 'self', '.', '_fit', '(', 'X', ')']


All attention probs:

[0.008487362414598465, 0.009466126561164856, 0.030226657167077065, 0.01010599173605442, 0.012624004855751991, 0.006936968304216862, 0.02028459869325161, 0.006467961240559816, 0.005130569450557232, 0.006041062064468861, 0.006941856350749731, 0.009787369519472122, 0.006438124924898148, 0.005957994144409895, 0.0068818554282188416, 0.005676357541233301, 0.005326216574758291, 0.0049581462517380714, 0.011775054037570953, 0.005011462140828371, 0.004647828172892332, 0.006164092104882002, 0.0037563249934464693, 0.006675753276795149, 0.0037664580158889294, 0.003520783269777894, 0.003161862026900053, 0.003496855730190873, 0.003550982801243663, 0.003685511415824294, 0.003105758223682642, 0.010549548082053661, 0.004237047862261534, 0.014240888878703117, 0.004127743653953075, 0.0028529679402709007, 0.0034004379995167255, 0.002750141080468893, 0.008301324211061, 0.00400784844532609, 0.019531728699803352, 0.004166257102042437, 0.00460787583142519, 0.003172059776261449, 0.003840676974505186, 0.0023927895817905664, 0.002693236107006669, 0.0019171069143339992, 0.0028480011969804764, 0.003362288000062108, 0.0027777976356446743, 0.001841197838075459, 0.0017732843989506364, 0.00839674286544323, 0.0018410879420116544, 0.0019357689889147878, 0.0017778952606022358, 0.0025891547556966543, 0.0018349898746237159, 0.0021509018260985613, 0.0016716463724151254, 0.0017453545005992055, 0.009021816775202751, 0.0014966739108785987, 0.001959602814167738, 0.0017039870144799352, 0.004288450349122286, 0.0019087102264165878, 0.00250831781886518, 0.0017084022983908653, 0.0076101068407297134, 0.0015673410380259156, 0.002350154099985957, 0.00185331457760185, 0.003002698067575693, 0.0016575607005506754, 0.0018537705764174461, 0.0029686109628528357, 0.0015265163965523243, 0.001563881291076541, 0.0015632386785000563, 0.001671183155849576, 0.0020373112056404352, 0.0020585511811077595, 0.0024870112538337708, 0.001695747603662312, 0.007907678373157978, 0.002080619568005204, 0.002277781255543232, 0.0016879730392247438, 0.0028595367912203074, 0.0015275100013241172, 0.0015732146566733718, 0.0019229332683607936, 0.002175030065700412, 0.0028149152640253305, 0.0013533829478546977, 0.0020584824960678816, 0.002037766855210066, 0.003148645395413041, 0.0016356294509023428, 0.0020750912372022867, 0.0018160418840125203, 0.004957838915288448, 0.0019840807653963566, 0.0049570039846003056, 0.0012306750286370516, 0.0021237472537904978, 0.0018693830352276564, 0.004032381344586611, 0.0013017542660236359, 0.004743258003145456, 0.0018043502932414412, 0.0017098371172323823, 0.002306894864886999, 0.008204985409975052, 0.0016055473824962974, 0.0038818649481981993, 0.0012093530967831612, 0.0016878907335922122, 0.0016691092168912292, 0.001576791051775217, 0.002281945664435625, 0.0017252630786970258, 0.0030376457143574953, 0.0014903618721291423, 0.0018642157083377242, 0.0028396162670105696, 0.0017429206054657698, 0.001459767809137702, 0.002713072346523404, 0.0024838410317897797, 0.005213635042309761, 0.002030519535765052, 0.0020781271159648895, 0.0017983962316066027, 0.003904755460098386, 0.0010058225598186255, 0.0033508006017655134, 0.001696993364021182, 0.0015503497561439872, 0.0019665907602757215, 0.002011415082961321, 0.004413405898958445, 0.0019327481277287006, 0.012024089694023132, 0.001523192971944809, 0.002288477960973978, 0.0055685690604150295, 0.0012443165760487318, 0.004656948149204254, 0.0019269367912784219, 0.001978363376110792, 0.0019997816998511553, 0.0026349069084972143, 0.005324463360011578, 0.0014789077686145902, 0.002991983899846673, 0.0020907651633024216, 0.002572267549112439, 0.0014666684437543154, 0.0021742244716733694, 0.0018695044564083219, 0.010993396863341331, 0.0013571163872256875, 0.0019516067113727331, 0.0015089398948475718, 0.0037111802957952023, 0.0016067271353676915, 0.00347259733825922, 0.001252081710845232, 0.001568490988574922, 0.001223691157065332, 0.002957924036309123, 0.0025648458395153284, 0.011807842180132866, 0.0022751339711248875, 0.0028763525187969208, 0.0019841897301375866, 0.012244120240211487, 0.0013224241556599736, 0.001799768186174333, 0.0014842665987089276, 0.0022269694600254297, 0.0020470351446419954, 0.0029227021150290966, 0.0027864519506692886, 0.0026720587629824877, 0.0041267466731369495, 0.0036231570411473513, 0.0033373255282640457, 0.0031914699357002974, 0.010179456323385239, 0.0026734641287475824, 0.006407284177839756, 0.001723417197354138, 0.0028142055962234735, 0.0015623002545908093, 0.0018722179811447859, 0.0013649179600179195, 0.009273494593799114, 0.001533670467324555, 0.0019002975896000862, 0.001753907185047865, 0.0026910067535936832, 0.001181526924483478, 0.0021121343597769737, 0.001640954869799316, 0.012654643505811691, 0.0015999340685084462, 0.0019029993563890457, 0.0015177993336692452, 0.009120571427047253, 0.0015624648658558726, 0.0015493424143642187, 0.00428308080881834, 0.001594832632690668, 0.0021155576687306166, 0.0013039857149124146, 0.0024049056228250265, 0.00495658814907074, 0.0011197252897545695, 0.002699189120903611, 0.001296937232837081, 0.002091017086058855, 0.0022246153093874454, 0.009871163405478, 0.002366230823099613, 0.0027571653481572866, 0.0023661181330680847, 0.0022503791842609644, 0.0016496372409164906, 0.0013990900479257107, 0.03899505361914635, 0.001118235057219863, 0.0036964397877454758, 0.0014429684961214662, 0.0015148433158174157, 0.002060935366898775, 0.0019626596476882696, 0.004240769427269697, 0.0011019554222002625, 0.002810470527037978, 0.001411915523931384, 0.00793147087097168, 0.0010121301747858524, 0.0024380977265536785, 0.0014631296508014202, 0.00278920098207891, 0.0019164824625477195, 0.002732610097154975, 0.005467367824167013, 0.001958159962669015, 0.0021917608100920916, 0.004886251408606768, 0.00842660665512085, 0.0018332242034375668, 0.0017629467183724046, 0.0011543557047843933, 0.00198762072250247, 0.0021269156131893396, 0.001294981804676354, 0.001892767962999642, 0.002228815108537674, 0.002335630590096116, 0.005451550707221031, 0.0009810716146603227, 0.0026856951881200075, 0.001581141259521246, 0.011386772617697716, 0.0018159005558118224]


Top-k source tokens:

['y', 'self', 'y', 'y', 'y', 'y', 'X', 'self', 'y', 'k']


Top-k attention probs:

[0.03899505361914635, 0.030226657167077065, 0.02028459869325161, 0.019531728699803352, 0.014240888878703117, 0.012654643505811691, 0.012624004855751991, 0.012244120240211487, 0.012024089694023132, 0.011807842180132866]
