
Original sample:

{"has_bug": true, "bug_kind": 1, "bug_kind_name": "VARIABLE_MISUSE", "source_tokens": ["#NEWLINE#", "def __init__(", "self", ",", "starting_mode", ",", "matchers", ",", "default_types", ")", ":", "#NEWLINE#", "#INDENT#", "'Initialize the tokenizer.\\n\\n    Args:\\n      starting_mode: Mode to start in.\\n      matchers: Dictionary of modes to sequences of matchers that defines the\\n          patterns to check at any given time.\\n      default_types: Dictionary of modes to types, defining what type to give\\n          non-matched text when in the given mode.  Defaults to Type.NORMAL.\\n    '", "#NEWLINE#", "self", ".", "__starting_mode", "=", "starting_mode", "#NEWLINE#", "self", ".", "matchers", "=", "matchers", "#NEWLINE#", "self", ".", "default_types", "=", "self"], "error_location": [31], "repair_targets": [8, 29], "repair_candidates": [4, 19, 8, 29, 6, 23, 25, 2, 15, 21, 27, 31], "provenances": [{"datasetProvenance": {"datasetName": "ETHPy150Open", "filepath": "google/closure-linter/closure_linter/common/tokenizer.py", "license": "apache-2.0", "note": "license: bigquery_api"}}], "txt_file": "eval__VARIABLE_MISUSE__SStuB.txt-00000-of-00300", "js_count": 935, "results": {"model": "transformer", "prob": {"loc": [[9.720191883388907e-05, 8.437919252202164e-10, 1.0223128593667141e-10, 1.1021290130530659e-10, 8.271559548411744e-10, 1.4022484562392634e-10, 1.0819510842807745e-09, 1.271643623734775e-10, 2.1977919928417577e-08, 4.4418572986426597e-11, 2.5794300029247097e-10, 2.785094932455934e-10, 1.632881885260673e-10, 1.3706620560771654e-10, 1.492219126042471e-10, 1.5435352648807132e-11, 9.89216764057943e-11, 1.571269373293327e-12, 1.0194661087536971e-10, 5.873926056665368e-07, 6.324062384877038e-10, 1.3950243737959056e-10, 1.3466709691822842e-10, 1.0232376057572878e-10, 1.7135097785114795e-09, 4.593928508711542e-07, 9.375122900223687e-10, 6.572135613502894e-10, 1.6095574872920793e-10, 3.1657168553245185e-10, 6.107252037423905e-09, 0.9999016523361206]], "pointer": [[0.0, 0.0, 9.1209230390632e-08, 0.0, 1.1155298125231639e-05, 0.0, 1.038528534991201e-05, 0.0, 0.01697167195379734, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.72630681258579e-08, 0.0, 0.0, 0.0, 1.056179854685979e-08, 0.0, 1.3619101082440466e-06, 0.0, 6.382127253345971e-07, 0.0, 3.831451600433411e-09, 0.0, 1.617770778450378e-10, 0.0, 0.9830046892166138, 0.0, 2.305350577103127e-09]], "target": [0.9999763369560242]}, "loss": [9.83428253675811e-05, 2.3663324100198224e-05], "acc": [0.0, 1.0, 1.0, 1.0]}}


All source tokens:

['#NEWLINE#', 'def __init__(', 'self', ',', 'starting_mode', ',', 'matchers', ',', 'default_types', ')', ':', '#NEWLINE#', '#INDENT#', "'Initialize the tokenizer.\\n\\n    Args:\\n      starting_mode: Mode to start in.\\n      matchers: Dictionary of modes to sequences of matchers that defines the\\n          patterns to check at any given time.\\n      default_types: Dictionary of modes to types, defining what type to give\\n          non-matched text when in the given mode.  Defaults to Type.NORMAL.\\n    '", '#NEWLINE#', 'self', '.', '__starting_mode', '=', 'starting_mode', '#NEWLINE#', 'self', '.', 'matchers', '=', 'matchers', '#NEWLINE#', 'self', '.', 'default_types', '=', 'self']


All attention probs:

[0.024346044287085533, 0.02564583718776703, 0.060645442456007004, 0.02922064997255802, 0.03923816233873367, 0.02558780275285244, 0.05592348799109459, 0.02329651080071926, 0.07207994163036346, 0.02194964699447155, 0.02233155257999897, 0.022724205628037453, 0.016529599204659462, 0.019405623897910118, 0.018135784193873405, 0.03717148303985596, 0.010904873721301556, 0.018057869747281075, 0.010762311518192291, 0.03408654406666756, 0.016397975385189056, 0.03473599627614021, 0.009740526787936687, 0.024267584085464478, 0.016966411843895912, 0.049037784337997437, 0.018202319741249084, 0.04694938659667969, 0.012758865021169186, 0.032509658485651016, 0.031596388667821884, 0.11879371851682663]


Top-k source tokens:

['self', 'default_types', 'self', 'matchers', 'matchers', 'self', 'starting_mode', 'self', 'self', 'starting_mode']


Top-k attention probs:

[0.11879371851682663, 0.07207994163036346, 0.060645442456007004, 0.05592348799109459, 0.049037784337997437, 0.04694938659667969, 0.03923816233873367, 0.03717148303985596, 0.03473599627614021, 0.03408654406666756]
