
Original sample:

{"has_bug": true, "bug_kind": 1, "bug_kind_name": "VARIABLE_MISUSE", "source_tokens": ["#NEWLINE#", "def predict(", "self", ",", "testing_data", ")", ":", "#NEWLINE#", "#INDENT#", "'\\n        Apply the model on the given data\\n        :param data:\\n        :return:\\n        '", "#NEWLINE#", "testing_data", "=", "transform_data", "(", "testing_data", ",", "self", ".", "num_cols", ",", "output", ".", "class_column", ",", "self", ".", "stats_info", ",", "self", ".", "class_func", ")", "#NEWLINE#", "feature_set", "=", "testing_data", "[", "self", ".", "num_cols", "]", ".", "as_matrix", "(", ")", "#NEWLINE#", "label_array", "=", "testing_data", "[", "[", "self", ".", "class_column", "]", "]", ".", "as_matrix", "(", ")", "#NEWLINE#", "l1", "=", "nonlin", "(", "np", ".", "dot", "(", "feature_set", ",", "self", ".", "syn0", ")", ")", "#NEWLINE#", "output", "=", "list", "(", "pd", ".", "DataFrame", "(", "l1", ")", "[", "0", "]", ".", "map", "(", "(", "lambda", "x", ":", "get_class", "(", "x", ")", ")", ")", ")", "#NEWLINE#", "target", "=", "list", "(", "pd", ".", "DataFrame", "(", "label_array", ")", "[", "0", "]", ")", "#NEWLINE#", "self", ".", "predict_summary", "[", "'output'", "]", "=", "output", "#NEWLINE#", "self", ".", "predict_summary", "[", "'target'", "]", "=", "target", "#NEWLINE#", "print", "(", "'Testing done.'", ")", "#NEWLINE#", "self", ".", "summary", "(", ")"], "error_location": [21], "repair_targets": [2, 17, 25, 29, 38, 52, 72, 121, 130, 144], "repair_candidates": [4, 11, 15, 36, 49, 47, 114, 34, 70, 21, 78, 128, 106, 137, 62, 86, 2, 17, 25, 29, 38, 52, 72, 121, 130, 144], "provenances": [{"datasetProvenance": {"datasetName": "ETHPy150Open", "filepath": "LargePanda/LearnPy/learnpy/models/NeuralNetwork.py", "license": "mit", "note": "license: bigquery_api"}}], "txt_file": "eval__VARIABLE_MISUSE__SStuB.txt-00000-of-00300", "js_count": 2389, "results": {"model": "transformer", "prob": {"loc": [[9.455038525629789e-05, 5.396745828001315e-10, 5.649630541171291e-09, 1.2367926127687667e-11, 2.6118780471762193e-09, 1.4954178520487638e-11, 1.741720309644812e-11, 1.812955167934671e-11, 1.4297600429979962e-11, 4.381441390255425e-11, 1.54367629789931e-11, 1.3766728867992128e-09, 2.2453634437868963e-11, 5.387404758416814e-11, 7.20551535104974e-11, 4.2275988221263106e-07, 3.1669653011157095e-11, 1.5782549667164858e-07, 6.472325453366068e-11, 5.3998940735544565e-11, 6.20217835423098e-11, 0.9998996257781982, 2.5176405404891966e-10, 3.040886639715801e-11, 1.4869203091016914e-10, 1.0663447369552159e-07, 5.4659436699022734e-11, 4.3329902166266976e-11, 3.265669679120009e-11, 2.4789500230326666e-07, 4.3749528305658814e-11, 8.66089144846427e-12, 2.562842819908706e-11, 2.2995402457204328e-11, 8.698977649324036e-10, 1.852853287465095e-11, 8.314523824992648e-07, 4.2693872742693983e-11, 4.5918955038359854e-07, 1.8987686423166394e-11, 1.3449502796192281e-11, 1.4642131257458502e-11, 1.709828459373064e-11, 5.174416884551336e-12, 1.3698202294687434e-10, 1.0143860751377076e-10, 4.4630542317403155e-11, 1.775958408112288e-10, 1.042827543068503e-11, 1.9325136690895306e-06, 4.072292850154646e-11, 1.0535761152397072e-10, 2.3157355144576286e-07, 6.041091932251774e-11, 1.386363072952701e-11, 1.3923999973852741e-11, 2.6910052311479582e-11, 3.33739182123427e-11, 4.6093814529135635e-12, 5.520649493107044e-10, 6.607311225925727e-11, 5.133652183908488e-11, 5.10778197515549e-10, 4.7025466853867215e-11, 2.6655348067672335e-11, 2.669605335403613e-11, 1.016722053770458e-10, 4.1275007717223033e-11, 1.396687713400846e-11, 7.054001827100365e-11, 3.619700521539926e-07, 1.5912689568797767e-11, 2.5498462719042436e-07, 5.4151173128902386e-11, 8.425563025049598e-11, 1.2396320081542456e-10, 1.2457994358339164e-10, 1.9449696020412688e-10, 4.715446366709841e-10, 2.4948088733767193e-11, 7.009219593623328e-11, 6.056136148124835e-11, 1.0043327280939707e-10, 1.171032715241438e-10, 1.7898182935738305e-11, 7.694819087467053e-11, 5.669560820820152e-08, 7.019776426808733e-11, 1.3029746726012092e-10, 4.6023493349700573e-10, 1.5471936232192007e-10, 8.489945552137002e-11, 8.350668247170123e-12, 9.912847626081245e-11, 5.683998827166192e-10, 1.8913031141210013e-09, 2.3781410174450457e-10, 1.4453264973735003e-10, 6.207968028526523e-10, 2.611936500418466e-10, 2.1412499506823224e-07, 2.623785633204534e-10, 2.9092056541557554e-10, 2.767767681710609e-10, 1.4316819951787352e-10, 2.3724597286722826e-10, 1.170487734514225e-10, 2.2532186186308145e-11, 7.684053393575141e-11, 3.515936500275707e-11, 1.4114306945423039e-10, 1.6544061953727152e-10, 1.708556907065173e-11, 2.1242876901972352e-10, 1.5862026714330568e-07, 1.5551671062041805e-10, 4.697385813656751e-10, 2.532422271883661e-09, 4.606942882734444e-10, 2.7217925135936127e-10, 1.410690592118513e-10, 1.0360704294498646e-08, 7.26898541358878e-11, 2.592405110024565e-11, 3.6890038246362167e-10, 1.7439991673029454e-09, 8.880888385798258e-11, 4.4780551489154163e-10, 3.038988154457911e-07, 1.4520380731131155e-10, 3.262392134217862e-09, 5.518351747779704e-11, 2.396691178851995e-11, 9.14591902212436e-10, 1.0824524609986952e-09, 7.169023014119702e-11, 7.330473450473107e-10, 1.1066912009027874e-07, 1.0330484384590832e-10, 2.3703010040843076e-11, 9.82031470031508e-11, 5.275957892791894e-09, 2.0798850142167424e-10, 7.739550667018591e-11, 2.779183272405561e-10, 5.884381176768372e-11, 4.905100978169763e-11, 4.863442981672961e-10, 3.1779795461872595e-10]], "pointer": [[0.0, 0.0, 0.07117339223623276, 0.0, 0.00031781630241312087, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.419536253408296e-06, 0.0, 0.0, 0.0, 1.468867026233056e-06, 0.0, 0.1604052484035492, 0.0, 0.0, 0.0, 2.5438346984429927e-09, 0.0, 0.0, 0.0, 0.2814657688140869, 0.0, 0.0, 0.0, 0.30581215023994446, 0.0, 0.0, 0.0, 0.0, 2.9302198072400643e-06, 0.0, 4.4411328303795017e-07, 0.0, 0.15696901082992554, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.374396219944174e-06, 0.0, 3.434277004998876e-07, 0.0, 0.0, 0.017088571563363075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.500950687244767e-07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.217158095480954e-08, 0.0, 0.006725772749632597, 0.0, 0.0, 0.0, 0.0, 0.0, 2.5536843395457254e-07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.3973197710015484e-08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8896906201225647e-07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.49243172954084e-09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0997209756169468e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.224813553501463e-09, 0.0, 6.2842022998665925e-06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.972198622421047e-10, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.2572881132655311e-05, 0.0, 0.0, 0.0, 0.0]], "target": [0.9996697902679443]}, "loss": [0.00010048838157672435, 0.00033026427263394], "acc": [0.0, 1.0, 1.0, 1.0]}}


All source tokens:

['#NEWLINE#', 'def predict(', 'self', ',', 'testing_data', ')', ':', '#NEWLINE#', '#INDENT#', "'\\n        Apply the model on the given data\\n        :param data:\\n        :return:\\n        '", '#NEWLINE#', 'testing_data', '=', 'transform_data', '(', 'testing_data', ',', 'self', '.', 'num_cols', ',', 'output', '.', 'class_column', ',', 'self', '.', 'stats_info', ',', 'self', '.', 'class_func', ')', '#NEWLINE#', 'feature_set', '=', 'testing_data', '[', 'self', '.', 'num_cols', ']', '.', 'as_matrix', '(', ')', '#NEWLINE#', 'label_array', '=', 'testing_data', '[', '[', 'self', '.', 'class_column', ']', ']', '.', 'as_matrix', '(', ')', '#NEWLINE#', 'l1', '=', 'nonlin', '(', 'np', '.', 'dot', '(', 'feature_set', ',', 'self', '.', 'syn0', ')', ')', '#NEWLINE#', 'output', '=', 'list', '(', 'pd', '.', 'DataFrame', '(', 'l1', ')', '[', '0', ']', '.', 'map', '(', '(', 'lambda', 'x', ':', 'get_class', '(', 'x', ')', ')', ')', ')', '#NEWLINE#', 'target', '=', 'list', '(', 'pd', '.', 'DataFrame', '(', 'label_array', ')', '[', '0', ']', ')', '#NEWLINE#', 'self', '.', 'predict_summary', '[', "'output'", ']', '=', 'output', '#NEWLINE#', 'self', '.', 'predict_summary', '[', "'target'", ']', '=', 'target', '#NEWLINE#', 'print', '(', "'Testing done.'", ')', '#NEWLINE#', 'self', '.', 'summary', '(', ')']


All attention probs:

[0.010965515859425068, 0.008589296601712704, 0.040208589285612106, 0.013382169418036938, 0.024332528933882713, 0.011973300017416477, 0.007462893147021532, 0.007471990305930376, 0.007035974413156509, 0.009726382791996002, 0.008995557203888893, 0.021773487329483032, 0.007156672887504101, 0.006212178617715836, 0.005704614333808422, 0.017417846247553825, 0.005896620452404022, 0.01527873519808054, 0.005367249250411987, 0.006364917848259211, 0.006142118480056524, 0.056930236518383026, 0.00479566166177392, 0.0053616841323673725, 0.005454922094941139, 0.014943709596991539, 0.0032966521102935076, 0.0034047511871904135, 0.0039358967915177345, 0.016929034143686295, 0.0034594940952956676, 0.004849999211728573, 0.004316102247685194, 0.005363551899790764, 0.014541707932949066, 0.004445809405297041, 0.012888511642813683, 0.004276353400200605, 0.01455676555633545, 0.0027506633196026087, 0.0033769577275961637, 0.00482611358165741, 0.004064145963639021, 0.00434114271774888, 0.0032673252280801535, 0.0033360577654093504, 0.005346174351871014, 0.010486955754458904, 0.00434142118319869, 0.010712775401771069, 0.0027245248202234507, 0.0027258151676505804, 0.012207809835672379, 0.0023912647739052773, 0.0027938946150243282, 0.002857790794223547, 0.003707350231707096, 0.002601612824946642, 0.0035258298739790916, 0.0038647674955427647, 0.003510389244183898, 0.004207830410450697, 0.011241614818572998, 0.0036039683036506176, 0.004495303146541119, 0.0032484258990734816, 0.005286980886012316, 0.0028656579088419676, 0.0036624560598284006, 0.003616185626015067, 0.01848331280052662, 0.0036956474650651217, 0.009523914195597172, 0.002356220968067646, 0.004061976447701454, 0.0023118825629353523, 0.002154130022972822, 0.0053191217593848705, 0.01041181106120348, 0.007452226709574461, 0.004150798078626394, 0.0035649300552904606, 0.003957836888730526, 0.002284272573888302, 0.002589528216049075, 0.0034641819074749947, 0.015143276192247868, 0.0039476053789258, 0.003579408163204789, 0.004348629619926214, 0.0033498588018119335, 0.0021797737572342157, 0.003138052998110652, 0.0028469294775277376, 0.0033251282293349504, 0.0062585677951574326, 0.0094890296459198, 0.004236010834574699, 0.00902869924902916, 0.0036486038006842136, 0.01429292093962431, 0.003153118072077632, 0.0034770267084240913, 0.004437512252479792, 0.003240227932110429, 0.005066896788775921, 0.012203805148601532, 0.005948703736066818, 0.003413138445466757, 0.003232268150895834, 0.004610131494700909, 0.0030100992880761623, 0.0036053755320608616, 0.0034722897689789534, 0.017229551449418068, 0.002512318780645728, 0.002909825649112463, 0.004480625502765179, 0.002641905564814806, 0.002320793690159917, 0.004602926317602396, 0.007699552457779646, 0.0027357928920537233, 0.0038827580865472555, 0.004788519814610481, 0.007343251723796129, 0.006434047129005194, 0.006327683571726084, 0.017078807577490807, 0.005835395306348801, 0.008592711761593819, 0.002768872305750847, 0.0040322113782167435, 0.0038868137635290623, 0.004399022087454796, 0.0028643212281167507, 0.00283328746445477, 0.012276153080165386, 0.003913276828825474, 0.0043484424240887165, 0.00350549235008657, 0.008311240933835506, 0.003276212140917778, 0.0044477349147200584, 0.0070598311722278595, 0.002533708931878209, 0.005815867800265551, 0.005074812564998865, 0.00457266578450799]


Top-k source tokens:

['output', 'self', 'testing_data', 'testing_data', 'feature_set', 'testing_data', 'label_array', 'output', 'self', 'self']


Top-k attention probs:

[0.056930236518383026, 0.040208589285612106, 0.024332528933882713, 0.021773487329483032, 0.01848331280052662, 0.017417846247553825, 0.017229551449418068, 0.017078807577490807, 0.016929034143686295, 0.01527873519808054]
